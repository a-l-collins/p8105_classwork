---
title: "lecture 19 cross validation"
date: "2025-11-11"
output: github_document
---

```{r include = FALSE}
library(tidyverse)
library(modelr)
library(mgcv)

library(p8105.datasets)

set.seed(1)
```

### CV by "hand"

This code chunk imports data that is non-linear and shows increasing variance as the predictor increases:

```{r}
data("lidar")

lidar_df <- 
  lidar %>% 
  as_tibble() %>% 
  mutate(id = row_number())

lidar_df %>% 
  ggplot(aes(x = range, y = logratio)) + 
  geom_point() +
  theme_minimal()
```

Splitting this data into training and testing sets using `anti_join`:

```{r}
train_df <- sample_frac(lidar_df, size = .8)
test_df <- anti_join(lidar_df, train_df, by = "id")

ggplot(train_df, aes(x = range, y = logratio)) + 
  geom_point() + 
  geom_point(data = test_df, color = "red") +
  theme_minimal()
```

Fitting three different models to the training data for future testing:

```{r}
linear_mod <- lm(logratio ~ range, data = train_df)
smooth_mod <- mgcv::gam(logratio ~ s(range), data = train_df)
wiggly_mod <- mgcv::gam(logratio ~ s(range, k = 30), sp = 10e-6, data = train_df)
```

What the smooth and wiggly mods look like graphically:

```{r}
train_df %>% 
  add_predictions(smooth_mod) %>% 
  ggplot(aes(x = range, y = logratio)) + 
  geom_point() + 
  geom_line(aes(y = pred), color = "red") +
  theme_minimal()

train_df %>% 
  add_predictions(wiggly_mod) %>% 
  ggplot(aes(x = range, y = logratio)) + 
  geom_point() + 
  geom_line(aes(y = pred), color = "red") +
  theme_minimal()
```

`gather_predictions` can be used to put all models into a single dataset in an easily plottable manner:

```{r}
train_df %>% 
  gather_predictions(linear_mod, smooth_mod, wiggly_mod) %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = range, y = logratio)) + 
  geom_point() + 
  geom_line(aes(y = pred), color = "red") + 
  facet_wrap(~model) +
  theme_minimal()
```

Visual inspection says that the linear model is too simple, the wiggly model is too complex, and the smooth model is about right.

- The linear model will not be able to reproduce the actual complexity of the data spread
- The wiggly model is chasing datapoints and will change between training sets, which does not create reproducibility

Computing root mean squared errors (RMSEs) for each model on the *testing* dataset:

```{r}
rmse(linear_mod, test_df)
rmse(smooth_mod, test_df)
rmse(wiggly_mod, test_df)
```

### CV using `modelr`

`modelr` lets us automate the process, as well as simple ways to reiterate the whole process.

```{r}
cv_df <- crossv_mc(lidar_df, 100) 
```

When using non-`lm` models (ie when using `gam`), `cross_cv` products need to be coerced into datasets we can work with. This loses `cross_cv`'s smart memory storage:

```{r}
cv_df %>% pull(train) %>% nth(1) %>% as_tibble()

cv_df %>% pull(test) %>% nth(1) %>% as_tibble()

cv_df <-
  cv_df %>%  
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```

Candidate models and RMSEs can now be fitted using `map`:

```{r warning = FALSE}
cv_df <-
  cv_df %>%  
  mutate(
    linear_mod  = map(train, \(df) lm(logratio ~ range, data = df)),
    smooth_mod  = map(train, \(df) gam(logratio ~ s(range), data = df)),
    wiggly_mod  = map(train, \(df) gam(logratio ~ s(range, k = 30), sp = 10e-6, data = df))) %>% 
  mutate(
    rmse_linear = map2_dbl(linear_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_smooth = map2_dbl(smooth_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_wiggly = map2_dbl(wiggly_mod, test, \(mod, df) rmse(model = mod, data = df)))
```

We can now plot RMSEs to compare models:

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() + theme_minimal()
```

