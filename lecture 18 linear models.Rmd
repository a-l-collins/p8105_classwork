---
title: "lecture 18 linear models"
date: "2025-11-06"
output: github_document
---

```{r include = FALSE}
library(tidyverse)
library(p8105.datasets)

set.seed(1)
```

### Model Fitting

The code below loads and cleans the `airbnb` data, which will be used as the primary example for fitting linear models:

```{r}
data("nyc_airbnb")

nyc_airbnb <- 
  nyc_airbnb %>% 
  mutate(stars = review_scores_location / 2) %>% 
  rename(
    borough = neighbourhood_group,
    neighborhood = neighbourhood) %>% 
  filter(borough != "Staten Island") %>% 
  select(price, stars, borough, neighborhood, room_type)
```

Fitting a model that considers price as an outcome that may depend on rating and boro:

```{r}
fit = lm(price ~ stars + borough, data = nyc_airbnb)
```

Model variants:

- Intercept-only model: `outcome ~ 1`
- No-intercept model: `outcome ~ 0 + ...`
- Model using all available predictors: `outcome ~ .`

R will automatically create indicator variables for categorical variables, and determine reference category based on factor level. As such, you need to be careful with your factors:

```{r}
nyc_airbnb <- 
  nyc_airbnb %>% 
  mutate(
    borough = fct_infreq(borough),
    room_type = fct_infreq(room_type))

fit = lm(price ~ stars + borough, data = nyc_airbnb)
```

Changing reference category won't change model fit, it will just impact interpretation

### Tidying Output

The output of `lm` is an object of class `lm`-- a list that isn't a dataframe, but which can be manipulated using other functions. Some common functions are (output excluded):

```{r eval = FALSE}
summary(fit)
summary(fit)$coef
coef(fit)
fitted.values(fit)
```

The `broom` package has quick functions for obtaining a quick summary of the model and for cleaning up the coefficient table:

```{r}
fit %>% broom::glance()
```

```{r}
fit %>% broom::tidy()
```

Both of these functions produce dataframes, which make it straightforward to include the results in subsequent tests:

```{r}
fit %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value) %>% 
  mutate(term = str_replace(term, "^borough", "Borough: ")) %>% 
  knitr::kable(digits = 3)
```

`broom::tidy()` works with a lot of things, including most of the functions for model fitting that you're likely to run into (survival, mixed models, additive models, ...)

### Diagnostics

Regression diagnostics can ID issues in model fit, esp pertaining to failures in model assumptions. Examining residuals and fitted values are therefore an important component of any modeling exercise.

`modelr` package can be used to add residuals and fitted values to a dataframe:

```{r}
modelr::add_residuals(nyc_airbnb, fit)

modelr::add_predictions(nyc_airbnb, fit)
```

This can also be used as part of a pipeline:

```{r warning = FALSE}
nyc_airbnb %>% 
  modelr::add_residuals(fit) %>% 
  ggplot(aes(x = borough, y = resid)) + geom_violin() + theme_minimal()
```

```{r warning = FALSE}
nyc_airbnb %>% 
  modelr::add_residuals(fit) %>%  
  ggplot(aes(x = stars, y = resid)) + geom_point() + theme_minimal()
```

Using the above plots, we can ID some issues with the data-- most notably, extreme outliers present in the data. There are a number of ways to deal with this, which are not the subject of this course.

### Hypothesis Testing

Model summaries include results of t-tests for single coefficients, and are the standard way of assessing statistical significance.

Testing multiple coefficients is somewhat more complicated. A useful approach is to use nested models, meaning that the terms in a simple “null” model are a subset of the terms in a more complex “alternative” model. The are formal tests for comparing the null and alternative models, even when several coefficients are added in the alternative model. Tests of this kind are required to assess the significance of a categorical predictor with more than two levels, as in the example below:

```{r}
fit_null <- lm(price ~ stars + borough, data = nyc_airbnb)
fit_alt <- lm(price ~ stars + borough + room_type, data = nyc_airbnb)
```

The test of interest is implemented in the `anova()` function which, of course, can be summarized using `broom::tidy()`:

```{r}
anova(fit_null, fit_alt) |> 
  broom::tidy()
```

This only works for *nested* models. Comparing non-nested models requires other methods

### Nesting Data

Using `nest()` to create a list column containing datasets and fit separate models to each

A non-nested example:

- Analyzing how star ratings and room types affect price differently in each boro

```{r}
nyc_airbnb |> 
  lm(price ~ stars * borough + room_type * borough, data = _) |> 
  broom::tidy() |> 
  knitr::kable(digits = 3)
```

- The native R pipe (`|>`) has to be used here; the tidyverse pipe (`%>%`) does not work

A nested example:

- Nesting within boros
- Fit boro-specific models associating price with room and rating type

```{r}
nest_lm_res =
  nyc_airbnb %>%  
  nest(data = -borough) %>% 
  mutate(
    models = map(data, \(df) lm(price ~ stars + room_type, data = df)),
    results = map(models, broom::tidy)) %>% 
  select(-data, -models) %>% 
  unnest(results)

nest_lm_res %>% 
  select(borough, term, estimate) %>% 
  mutate(term = fct_inorder(term)) %>% 
  pivot_wider(
    names_from = term, values_from = estimate) %>% 
  knitr::kable(digits = 3)
```

Fitting models to nested datasets is a way to perform stratified analyses. These have a tradeoff: stratified models make it easy to interpret covariate effects in each stratum, but don’t provide a mechanism for assessing the significance of differences across strata.

A more extreme example on the assessment of neighborhood effects in Manhattan, using a neighborhood-specific model:

```{r}
manhattan_airbnb <-
  nyc_airbnb %>% 
  filter(borough == "Manhattan")

manhattan_nest_lm_res <-
  manhattan_airbnb %>% 
  nest(data = -neighborhood) %>%  
  mutate(
    models = map(data, \(df) lm(price ~ stars + room_type, data = df)),
    results = map(models, broom::tidy)) %>% 
  select(-data, -models) %>% 
  unnest(results)

manhattan_nest_lm_res %>% 
  filter(str_detect(term, "room_type")) %>% 
  ggplot(aes(x = neighborhood, y = estimate)) + 
  geom_point() + 
  facet_wrap(~term) + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 80, hjust = 1))
```

With this many factor levels, fitting models or main interactions to each wouldn't be a great idea. Instead, it'd be better to use a mixed model, with random intercepts and slopes for each neighborhood:

```{r eval = FALSE}
manhattan_airbnb |> 
  lme4::lmer(price ~ stars + room_type + (1 + room_type | neighborhood), data = _) |> 
  broom.mixed::tidy()
```

- Tho this isn't working for me

### Binary Outcomes

Linear models are appropriate for continuous outcomes, but logistic regression is better for binary outcomes

Using data from the washington post on homicides in 50 large US cities:

```{r}
baltimore_df <-
  read_csv("./data/homicide-data.csv") %>% 
  filter(city == "Baltimore") %>% 
  mutate(
    resolved = as.numeric(disposition == "Closed by arrest"),
    victim_age = as.numeric(victim_age),
    victim_race = fct_relevel(victim_race, "White")) %>% 
  select(resolved, victim_age, victim_race, victim_sex)
```

Using this data, a regression model comparing victim demographics to whether a homicide is resolved can be done using `glm`:

```{r}
fit_logistic <-
  baltimore_df |> 
  glm(resolved ~ victim_age + victim_race + victim_sex, data = _, family = binomial()) 
```

This can then be tidied:

```{r}
fit_logistic %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>% 
  select(term, log_OR = estimate, OR, p.value) %>% 
  knitr::kable(digits = 3)
```

We can also compare fitted values:

```{r}
baltimore_df %>% 
  modelr::add_predictions(fit_logistic) %>% 
  mutate(fitted_prob = boot::inv.logit(pred))
```

